{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2130b2f2-bf4b-4faf-a01f-1b8d579f75d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309fda01-9e84-4b1e-9023-ac59c9b9c7ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "from sklearn.model_selection import KFold\n",
    "import joblib\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c57586-5c37-4df0-9fbb-5d993520e517",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.read_csv('../data/Activity.csv',skipinitialspace=True, na_values='?')\n",
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567516ec-acdc-40ea-be32-cd5f6f268955",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_isotopes = Y.columns.values.tolist()\n",
    "list_isotopes[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aa48f5-e1e1-4a2c-bb44-a21b5306517b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Để tải lại các bộ dữ liệu\n",
    "loaded_data = np.load('../data/hpge-soil-gamma-41.npz')\n",
    "x_train = loaded_data['x_train']\n",
    "y_train = loaded_data['y_train']\n",
    "x_test = loaded_data['x_test']\n",
    "y_test = loaded_data['y_test']\n",
    "\n",
    "print('x_train shape:\\t', x_train.shape)\n",
    "print('x_test shape:\\t', x_test.shape)\n",
    "print('y_train shape:\\t', y_train.shape)\n",
    "print('y_test shape:\\t', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694661ff-75c2-4206-bc7d-84c110afc29d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compare scaling methods for mlp inputs on regression problem\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, PoissonRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "\n",
    "# prepare dataset with input and output scalers, can be none\n",
    "def get_scaled_dataset(input_scaler, output_scaler, x_train, y_train, x_test, y_test, \n",
    "                       input_scaler_path='scaler/input_scaler_ann.sav'):\n",
    "    x_train = x_train\n",
    "    x_test = x_test\n",
    "    y_test = y_test\n",
    "    y_train = y_train\n",
    "\n",
    "    if input_scaler is not None:\n",
    "        # fit scaler\n",
    "        input_scaler.fit(x_train)\n",
    "        # transform training dataset\n",
    "        x_train = input_scaler.transform(x_train)\n",
    "        # transform test dataset\n",
    "        x_test = input_scaler.transform(x_test)\n",
    "        # save the scaler\n",
    "        # joblib.dump(input_scaler, input_scaler_path)\n",
    "    \n",
    "    if output_scaler is not None:\n",
    "        # fit scaler\n",
    "        output_scaler.fit(y_train)\n",
    "        # transform training dataset\n",
    "        y_train = output_scaler.transform(y_train)\n",
    "        # transform test dataset\n",
    "        y_test = output_scaler.transform(y_test)\n",
    "        # save the scaler\n",
    "        #joblib.dump(output_scaler, output_scaler_path)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, output_scaler\n",
    "    \n",
    "def percentage_loss(y_true, y_pred):\n",
    "    return ((y_pred - y_true) / y_true) * 100\n",
    "\n",
    "def custom_loss(true_values, predicted_values):\n",
    "    errors = []\n",
    "    for true_val, pred_val in zip(true_values, predicted_values):\n",
    "        loss = percentage_loss(true_val, pred_val)\n",
    "        errors.append(loss)\n",
    "    \n",
    "    return np.mean(np.abs(errors)), errors\n",
    "\n",
    "def thong_ke_gia_tri(danh_sach, nguong):\n",
    "    if not danh_sach:\n",
    "        return None\n",
    "\n",
    "    # Tính giá trị nhỏ nhất, lớn nhất và trung bình\n",
    "    min_value = min(danh_sach)\n",
    "    max_value = max(danh_sach)\n",
    "    average = sum(danh_sach) / len(danh_sach)\n",
    "    \n",
    "    count = 0\n",
    "    indices = []\n",
    "    for i, gia_tri in enumerate(danh_sach):\n",
    "        if gia_tri > nguong:\n",
    "            count += 1\n",
    "            indices.append(i)\n",
    "    \n",
    "    return min_value, max_value, average, count, indices\n",
    "\n",
    "def eval_results(y_pred, testy):\n",
    "    ab_errors = []\n",
    "    good_count = 0\n",
    "    for i in range(np.shape(testy)[0]):\n",
    "        ab_error = abs((testy[i]-y_pred[i])/testy[i]).tolist()[0]\n",
    "        if ab_error <= 0.15:\n",
    "            good_count = good_count +1\n",
    "        ab_errors.append(ab_error)\n",
    "    return   good_count/np.shape(testy)[0], ab_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec81b17-758e-4005-b79a-c72e9fe7932b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainX, trainy, testX, testy, output_scaler = get_scaled_dataset(MinMaxScaler(), None, x_train, y_train, x_test, y_test)\n",
    "input_shape = trainX.shape[1]\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de8c8e3-b3c4-449a-88a5-b2a6db4a2834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = []\n",
    "test = []\n",
    "for i in range(len(trainX)):\n",
    "    train.append([trainX[i],trainy[i]])\n",
    "\n",
    "for i in range(len(testX)):\n",
    "    test.append([testX[i],testy[i]])\n",
    "\n",
    "trainDataSet = torch.utils.data.DataLoader(train, batch_size=32)\n",
    "testDataSet = torch.utils.data.DataLoader(test, batch_size=16)\n",
    "x_train_torch = torch.tensor(trainX,device=device)\n",
    "y_train_torch = torch.tensor(trainy,device=device)\n",
    "x_test_torch = torch.tensor(testX,device=device)\n",
    "y_test_torch = torch.tensor(testy,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0fc8bd-9c59-4b08-a7fe-7b9c05fa0396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ParameterSampler, ParameterGrid\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "trainDataSet = torch.utils.data.DataLoader(train, batch_size=32)\n",
    "testDataSet = torch.utils.data.DataLoader(test, batch_size=16)\n",
    "x_train_torch = torch.tensor(trainX,device=device)\n",
    "y_train_torch = torch.tensor(trainy,device=device)\n",
    "x_test_torch = torch.tensor(testX,device=device)\n",
    "y_test_torch = torch.tensor(testy,device=device)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_shape, hiddenSize1, hiddenSize2):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape, hiddenSize1)\n",
    "        self.fc2 = nn.Linear(hiddenSize1, hiddenSize2)\n",
    "        self.fc3 = nn.Linear(hiddenSize2, 41)  # Output layer for regression\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def evaluate(self, dataset):\n",
    "        self.eval()\n",
    "        total_loss = 0\n",
    "        criterion = nn.MSELoss()\n",
    "        with torch.no_grad():\n",
    "            for x, y in dataset:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                output = self(x.float())\n",
    "                loss = criterion(output, y.float())\n",
    "                total_loss += loss.item()\n",
    "        return total_loss / len(dataset)\n",
    "\n",
    "    def train_model(self, trainDataset, valDataset, optimizer, criterion, num_epochs, model_path, best_loss):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        best_val_loss = best_loss\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in trainDataset:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs.float())\n",
    "                loss = criterion(outputs, labels.float())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            avg_train_loss = running_loss / len(trainDataset)\n",
    "            val_loss = self.evaluate(valDataset)\n",
    "            train_losses.append(avg_train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "            # Save the model with the lowest validation loss\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(self, model_path)\n",
    "                print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "        return val_losses, train_losses\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'hiddenSize1': [32, 64, 128],\n",
    "    'hiddenSize2': [32, 64, 128],\n",
    "    'lr': [0.001, 0.005, 0.01, 0.02],\n",
    "    'num_epochs': [200]\n",
    "}\n",
    "param_list = list(ParameterGrid(param_grid))\n",
    "input_shape = trainX.shape[1]\n",
    "best_loss = float('inf')\n",
    "best_params = {}\n",
    "model_path = \"models/best_mlp.pth\"\n",
    "\n",
    "results = []\n",
    "best_params = {}\n",
    "for params in param_list:\n",
    "    print(f\"Training at params: {params}\")\n",
    "    model = Net(input_shape, params['hiddenSize1'], params['hiddenSize2']).to(device)\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "    val_losses, train_losses = model.train_model(trainDataSet, testDataSet, optimizer, criterion, params['num_epochs'], model_path, best_loss)\n",
    "    \n",
    "    # Evaluate the final model on the validation set\n",
    "    val_loss = min(val_losses)\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_params = params\n",
    "        \n",
    "        print(f\"Best params: {best_params}\")\n",
    "        print(f\"Best validation loss: {best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71561955-250f-48ea-80e8-96df44722942",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best params: {best_params}\")\n",
    "print(f\"Best validation loss: {best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef04e978-caba-4229-a4a7-9adb0fa3bc01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = 'models/best_mlp.pth'\n",
    "model = torch.load(model_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac9467c-5007-4e75-ba53-edff70476f86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = model(x_test_torch.float())\n",
    "val_pre = outputs.cpu().detach().numpy()\n",
    "testy = y_test_torch.cpu().detach().numpy()\n",
    "MAE = mean_absolute_error(val_pre,testy)\n",
    "print(\"MAE: \",MAE)\n",
    "MSE = mean_squared_error(val_pre,testy)\n",
    "print(\"MSE: \",MSE)\n",
    "RMSE = np.sqrt(mean_squared_error(val_pre,testy))\n",
    "print(\"RMSE: \",RMSE)\n",
    "#Root Mean Squared Log Error(RMSLE)\n",
    "RMSLE = np.log(np.sqrt(mean_squared_error(val_pre,testy)))\n",
    "print(\"RMSLE: \",RMSLE)\n",
    "#R Squared (R2)\n",
    "R2_Score = r2_score(testy, val_pre)\n",
    "print(\"R2_Score: \",R2_Score)\n",
    "mape = mean_absolute_percentage_error(testy, val_pre)\n",
    "print(\"MAPE: \", mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40590c24-ef10-4439-a244-6c9e1eb261c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idxs = np.arange(np.shape(testy)[0])\n",
    "\n",
    "k = 300\n",
    "index = 2\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(idxs[0:k], val_pre[0:k, index], label='Ground Truth')\n",
    "plt.plot(idxs[0:k], testy[0:k, index], label='Predicted',  linestyle='--', color = \"red\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel(\"Output index\")\n",
    "plt.ylabel(\"Output value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c2f558-538b-4b0b-81ba-534b3e598edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(val_pre[0:k, index], testy[0:k, index])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e2cfb9-d6b3-4034-9dd9-f257c4bcb7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    epsilon = 1e-6  # Để tránh chia cho 0\n",
    "    # R2 Score\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    # MAE\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    # MSE\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    # RMSE\n",
    "    rmse = np.sqrt(mse)\n",
    "    # RMSLE\n",
    "    rmsle = np.log(rmse)\n",
    "    # MAPE\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + epsilon)))\n",
    "    # Count the number of samples with percentage error < 0.15\n",
    "    percentage_errors = np.abs((y_true - y_pred) / (y_true + epsilon))\n",
    "    count_below_threshold = np.sum(percentage_errors < 0.15)\n",
    "    percentage_below_threshold = count_below_threshold / len(y_true)\n",
    "    \n",
    "    return {\n",
    "        \"R2 Score\": r2,\n",
    "        \"MAE\": mae,\n",
    "        \"MSE\": mse,\n",
    "        \"RMSE\": rmse,\n",
    "        \"RMSLE\": rmsle,\n",
    "        \"MAPE\": mape,\n",
    "        \"Count < 0.15\": count_below_threshold,\n",
    "        \"Percentage Below Threshold\": percentage_below_threshold\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311291ad-2058-4fe2-8e6b-f759f91f22a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu kết quả đánh giá cho mỗi output\n",
    "results = []\n",
    "for i, output_name in enumerate(list_dongvi):\n",
    "    testy_output = y_test[:, i]\n",
    "    pre = val_pre[:, i]\n",
    "    metrics = calculate_metrics(testy_output, pre)\n",
    "    result = {\n",
    "        \"Output\": output_name,\n",
    "        **metrics\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv(\"Results/mlp_evaluation_results.csv\", encoding='utf-8', index=False)\n",
    "print(\"Kết quả đánh giá đã được lưu vào mlp_evaluation_results_canbang.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7101b39c-1049-467f-ac50-fcc64788cd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c9e1be-90c5-4bb1-bd79-b6cb5494a58c",
   "metadata": {},
   "source": [
    "## Deep CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1053213-d58d-427c-91f1-411d992f46fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_torch = torch.tensor(trainX, dtype=torch.float32).unsqueeze(1) \n",
    "y_train_torch = torch.tensor(trainy)\n",
    "x_test_torch = torch.tensor(testX, dtype=torch.float32).unsqueeze(1)\n",
    "y_test_torch = torch.tensor(testy)\n",
    "\n",
    "train_dataset = TensorDataset(x_train_torch, y_train_torch)\n",
    "test_dataset = TensorDataset(x_test_torch, y_test_torch)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0409dd-4eb1-4246-a539-9a6acc927e26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Deep_RegressionCNN1D(nn.Module):\n",
    "    def __init__(self, input_length, kernel_size, hiddenSize1, hiddenSize2, unit1, unit2):\n",
    "        super(Deep_RegressionCNN1D, self).__init__()\n",
    "        \n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        # Block 1\n",
    "        self.conv1_1 = nn.Conv1d(in_channels=1, out_channels=hiddenSize1, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv1_2 = nn.Conv1d(in_channels=hiddenSize1, out_channels=hiddenSize1, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=kernel_size, stride=2, padding=0)\n",
    "        \n",
    "        # Block 2\n",
    "        self.conv2_1 = nn.Conv1d(in_channels=hiddenSize1, out_channels=hiddenSize2, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2_2 = nn.Conv1d(in_channels=hiddenSize2, out_channels=hiddenSize2, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=kernel_size, stride=2, padding=0)\n",
    "        \n",
    "        # Block 3\n",
    "        self.conv3_1 = nn.Conv1d(in_channels=hiddenSize2, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3_2 = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=kernel_size, stride=2, padding=0)\n",
    "        \n",
    "        # Compute conv_output_size\n",
    "        conv_output_size = self.compute_conv_output_size(input_length)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * conv_output_size, unit1)  # Adjusted for the new conv output size\n",
    "        self.fc2 = nn.Linear(unit1, unit2)\n",
    "        self.fc3 = nn.Linear(unit2, 41)\n",
    "    \n",
    "    def compute_conv_output_size(self, length):\n",
    "        for _ in range(3):  # 3 pooling layers\n",
    "            length = (length - self.kernel_size) // 2 + 1\n",
    "        return length\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Block 1\n",
    "        x = F.relu(self.conv1_1(x))\n",
    "        x = F.relu(self.conv1_2(x))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Block 2\n",
    "        x = F.relu(self.conv2_1(x))\n",
    "        x = F.relu(self.conv2_2(x))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Block 3\n",
    "        x = F.relu(self.conv3_1(x))\n",
    "        x = F.relu(self.conv3_2(x))\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        # Flatten and fully connected layers\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def train_model(self, trainDataset, valDataset, optimizer, criterion, num_epochs, model_path, best_loss):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        best_val_loss = best_loss\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in trainDataset:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs.float())\n",
    "                loss = criterion(outputs, labels.float())\n",
    "                # loss = improved_custom_log_loss(labels.float().view(-1, 1), outputs)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            avg_train_loss = running_loss / len(trainDataset)\n",
    "            val_loss = self.evaluate(valDataset)\n",
    "            train_losses.append(avg_train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "            # Save the model with the lowest validation loss\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(self, model_path)\n",
    "                print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "        return val_losses, train_losses\n",
    "\n",
    "    def evaluate(self, valDataset):\n",
    "        self.eval()\n",
    "        val_loss = 0.0\n",
    "        criterion = nn.MSELoss()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valDataset:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = self(inputs.float())\n",
    "                loss = criterion(outputs, labels.float())\n",
    "                # loss = improved_custom_log_loss(outputs, labels.float().view(-1, 1))\n",
    "                val_loss += loss.item()\n",
    "        return val_loss / len(valDataset)\n",
    "\n",
    "# Grid search parameters\n",
    "param_grid = {\n",
    "    'hiddenSize1': [32, 64],\n",
    "    'hiddenSize2': [32, 64],\n",
    "    'lr': [0.001, 0.005, 0.01],\n",
    "    'num_epochs': [200],\n",
    "    'kernel_size': [2, 3, 4],\n",
    "    'unit1': [64, 128],\n",
    "    'unit2': [64, 128],\n",
    "}\n",
    "\n",
    "# Generate all combinations of parameters from the grid\n",
    "param_list = list(ParameterGrid(param_grid))\n",
    "\n",
    "input_length = trainX.shape[1]\n",
    "best_loss = float('inf')\n",
    "best_params = {}\n",
    "model_path = \"models/best_cnn.pth\"\n",
    "\n",
    "for params in param_list:\n",
    "    print(f\"Training at params: {params}\")\n",
    "    model = Deep_RegressionCNN1D(input_length=input_length, kernel_size=params['kernel_size'], \n",
    "                                 hiddenSize1=params['hiddenSize1'], hiddenSize2=params['hiddenSize2'],\n",
    "                                 unit1=params['unit1'], unit2=params['unit2']).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "    val_losses, train_losses = model.train_model(train_loader, test_loader, optimizer, criterion, params['num_epochs'], model_path, best_loss)\n",
    "    \n",
    "    # Evaluate the final model on the validation set\n",
    "    val_loss = min(val_losses)\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_params = params\n",
    "        \n",
    "        print(f\"Best params: {best_params}\")\n",
    "        print(f\"Best validation loss: {best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeba14a-ffe2-46e4-8e42-6a14ef8421a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_params)\n",
    "print(best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c86652-74d8-4dad-b0ae-6711925d0f8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hiddenSize1 = best_params[\"hiddenSize1\"]\n",
    "hiddenSize2 = best_params[\"hiddenSize2\"]\n",
    "kernel_size = best_params[\"kernel_size\"]\n",
    "\n",
    "model_path = \"models/best_cnn.pth\"\n",
    "model = torch.load(model_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f37173-5b4b-4573-bd58-7b32800a8291",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = model(x_test_torch.to(device))\n",
    "val_pre = outputs.cpu().detach().numpy()\n",
    "testy = y_test_torch.cpu().detach().numpy()\n",
    "MAE = mean_absolute_error(val_pre,testy)\n",
    "print(\"MAE: \",MAE)\n",
    "MSE = mean_squared_error(val_pre,testy)\n",
    "print(\"MSE: \",MSE)\n",
    "RMSE = np.sqrt(mean_squared_error(val_pre,testy))\n",
    "print(\"RMSE: \",RMSE)\n",
    "#Root Mean Squared Log Error(RMSLE)\n",
    "RMSLE = np.log(np.sqrt(mean_squared_error(val_pre,testy)))\n",
    "print(\"RMSLE: \",RMSLE)\n",
    "#R Squared (R2)\n",
    "R2_Score = r2_score(testy, val_pre)\n",
    "print(\"R2_Score: \",R2_Score)\n",
    "mape = mean_absolute_percentage_error(testy, val_pre)\n",
    "print(\"MAPE: \", mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24434d7b-2487-47c1-a50a-a6d558e5b1b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idxs = np.arange(np.shape(testy)[0])\n",
    "\n",
    "k = 300\n",
    "index = 2\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(idxs[0:k], val_pre[0:k, index], label='Ground Truth')\n",
    "plt.plot(idxs[0:k], testy[0:k, index], label='Predicted',  linestyle='--', color = \"red\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel(\"Output index\")\n",
    "plt.ylabel(\"Output value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f4e8d2-22d9-466b-9b92-7c5b089291fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(val_pre[0:k, index], testy[0:k, index])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DVPX",
   "language": "python",
   "name": "dvpx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
